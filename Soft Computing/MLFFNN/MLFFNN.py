from math import exp
from random import seed
from random import random

import csv


def initialise(l, m, n):
    network = list()
    hidden_layer = [{'weights':[random() for i in range(l + 1)]} for i in range(m)]
    network.append(hidden_layer)
    output_layer = [{'weights':[random() for i in range(m + 1)]} for i in range(n)]
    network.append(output_layer)
    return network
    
def transfer(activation):
    return 1.0 / (1.0 + exp(-activation))
    
def activate(weights, inputs):
    activation = weights[-1]
    for i in range(len(weights)-1):
        activation += weights[i] * inputs[i]
    return transfer(activation)

def forward(network, row):
    inputs = row
    for layer in network:
        new_inputs = list()
        for neuron in layer:
            neuron['output'] = activate(neuron['weights'], inputs)
            new_inputs.append(neuron['output'])
        inputs = new_inputs
    return inputs

def derivative(output):
    return output * (1.0 - output)

def backward(network, expected):
    for i in reversed(range(len(network))):
        layer = network[i]
        errors = list()
        if i != len(network)-1:
            for j in range(len(layer)):
                error = 0.0
                for neuron in network[i + 1]:
                    error += (neuron['weights'][j] * neuron['delta'])
                errors.append(error)
        else:
            for j in range(len(layer)):
                neuron = layer[j]
                errors.append(expected[j] - neuron['output'])
        for j in range(len(layer)):
            neuron = layer[j]
            neuron['delta'] = errors[j] * derivative(neuron['output'])

def update(network, row, alpha):
    for i in range(len(network)):
        inputs = row[:-1]
        if i != 0:
            inputs = [neuron['output'] for neuron in network[i - 1]]
        for neuron in network[i]:
            for j in range(len(inputs)):
                neuron['weights'][j] += alpha * neuron['delta'] * inputs[j]
        neuron['weights'][-1] += alpha * neuron['delta']

def train(l, m, n, data, alpha, epochs):
    network = initialise(l, m, n)
    for epoch in range(epochs):
        sum_error = 0
        for row in data:
            outputs = forward(network, row)
            expected = [0 for i in range(n)]
            expected[row[-1]] = 1
            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])
            backward(network, expected)
            update(network, row, alpha)
        if epoch%10 == 0:
    	    print('Epoch=%d, Alpha=%.3f, Error=%.3f' % (epoch, alpha, sum_error))
    return network

seed(1)
dataset = [[6.126227655013033, -9.563056152254353, 0], [-2.665673083795725, 4.873072694190313, 1], [-3.8386378715034213, 3.5188951975811653, 1], [5.651180554428492, -10.176419328440788, 0], [8.332264358409374, -9.991595502541694, 0], [7.260483533024018, -10.196035067988497, 0], [5.960741573344057, -11.735850829797915, 0], [-3.3638452375181505, 3.0546496707372848, 1], [-3.919506055522194, 3.1862557605956856, 1], [7.173041451633623, -10.344074610512376, 0], [6.5383193766299605, -10.499004070645558, 0], [7.03429731322128, -9.178962507376687, 0], [-2.625351079141991, 2.8729651926174764, 1], [-3.344367056282606, 3.4778551001621385, 1], [-3.141259632287167, 2.675608811166802, 1], [-3.9983767847528844, 4.3129693446115525, 1], [8.021453087285806, -7.01062846667827, 0], [8.234808102511426, -9.02215491128864, 0], [-3.5256052699333145, 2.8194256943585985, 1], [6.932238713074296, -9.066866094792637, 0], [6.503456528593125, -10.41389999326993, 0], [-2.035849151067414, 1.028624775212006, 1], [7.007008070930117, -10.733509399228554, 0], [-2.631085590813753, 2.905763162628671, 1], [-1.4707512165250285, 2.9884838666668356, 1], [-3.7748028645487985, 3.013629601781322, 1], [-2.0304395558608155, 1.521004081746682, 1], [6.600298734956448, -9.014316369862858, 0], [7.358390465961558, -10.204051432798524, 0], [6.664934993253028, -10.735835416184077, 0], [-3.1432719532197724, 4.8088291054155095, 1], [8.69051619818127, -9.974238065022922, 0], [7.247448146879545, -10.045554592958421, 0], [-2.6877619710432707, 3.4375402311887, 1], [7.831927013452764, -9.606723314288807, 0], [7.394801054460265, -9.019087978687311, 0], [5.8029116937663, -10.166657752602468, 0], [-4.508813571107174, 3.3546250181048536, 1], [-3.5245967364163526, 2.20754225228851, 1], [7.776209901638463, -10.563840278868984, 0], [-2.797877732629475, 5.435626408618962, 1], [-4.675050518894909, 3.65415881408866, 1], [7.4458137992505184, -10.3780706411793, 0], [-4.80542939779478, 3.043868787445407, 1], [7.7905967954370405, -10.09242765838703, 0], [-2.908178000843287, 2.3422919542016865, 1], [8.266941083837223, -10.101462247716768, 0], [8.226432280593459, -10.552839273600785, 0], [-2.79521764732817, 4.2668670109971725, 1], [7.649507905409761, -9.109340450059689, 0], [8.405460120075984, -11.167173935393075, 0], [-4.511060614595399, 3.921511940620575, 1], [7.192056129027889, -9.378823788410664, 0], [-3.034638128418865, 2.8615209161676667, 1], [6.151849277682738, -11.370277034703156, 0], [-3.898210307654131, 2.4605932339302647, 1], [7.406024720003287, -8.536681568403557, 0], [5.13812485983137, -7.901522530971667, 0], [-4.689916521912696, 4.624177543031035, 1], [7.635323834131962, -9.717792803769145, 0], [-4.4010098307620495, 3.214798162525173, 1], [-2.374945183611106, 3.8182745588711273, 1], [-3.6726822859526456, 4.436385967120974, 1], [8.695872691078533, -10.970650540158525, 0], [6.879123391427241, -7.258100156182845, 0], [7.433940436990643, -8.36282830033204, 0], [-2.3818848350421074, 4.6342853629964, 1], [5.798265627332074, -10.530421630053606, 0], [-3.730068672144109, 2.9890723524801137, 1], [-3.304263343382563, 2.622504624086461, 1], [-4.263174017957425, 4.238204367133659, 1], [-3.9217155956002046, 1.1322581920880808, 1], [6.609116203077129, -8.085944585775476, 0], [7.389091869704149, -10.630323626348707, 0], [7.69494564561482, -10.357878782767362, 0], [8.13040278839794, -10.098809639613691, 0], [6.459671194032925, -9.297822897824675, 0], [6.373500275110803, -9.089522478287925, 0], [-2.87204020720218, 3.4161480857934037, 1], [6.612145827513237, -11.370266546677566, 0], [-1.5756860079966235, 2.4950699679107284, 1], [-4.10523957491071, 3.064458374672253, 1], [6.693930254865067, -8.357970967312358, 0], [7.8781186171911814, -9.148449176265785, 0], [-3.286030310050316, 1.5590254382033963, 1], [-4.7497963056988155, 3.8867956058824755, 1], [-3.483559261809698, 2.946386277959511, 1], [-5.780234275756221, 3.6852004746169387, 1], [-3.48472408225335, 2.803529648760357, 1], [7.230432873683021, -9.726427779808752, 0], [-3.5800237372265373, 4.655524868262199, 1], [7.810797695948294, -10.42527268921801, 0], [8.406121670576933, -9.842542212963211, 0], [-3.7256260581969562, 4.959127772806544, 1], [-3.2970718735829267, 5.415918483289015, 1], [-4.223632424389002, 3.947771801944938, 1], [-2.843650668737289, 5.183583767768723, 1], [-3.2687527510934475, 4.462641970857163, 1], [7.265877611401136, -8.744482252485323, 0], [-2.702204269953195, 2.8938517391491647, 1]]

l = len(dataset[0]) - 1
n = len(set([row[-1] for row in dataset]))
network = train(l, 4, n, dataset, 0.5, 500)

print("\n")
for i, layer in enumerate(network):
    print("Layer " + str(i))
    for j, neuron in enumerate(layer):
        print("Neuron " + str(j))
        print(neuron)
        print("\n")
    print("\n")
